import requests
import json
import re
import os
import sys
from datetime import datetime

# <<< ëª¨ë“  print ì¶œë ¥ì„ íŒŒì¼ê³¼ ì½˜ì†”ë¡œ ë™ì‹œì— ë³´ë‚´ëŠ” í´ë˜ìŠ¤ >>>
class Tee:
    """
    ëª¨ë“  print ì¶œë ¥ì„ í„°ë¯¸ë„(stdout)ê³¼ ë¡œê·¸ íŒŒì¼ë¡œ ë™ì‹œì— ë³´ë‚´ëŠ” ì—­í• ì„ í•˜ëŠ” í´ë˜ìŠ¤.
    """
    def __init__(self, filename, mode='w', encoding='utf-8'):
        self.file = open(filename, mode, encoding=encoding)
        self.stdout = sys.stdout

    def write(self, message):
        self.stdout.write(message)
        self.file.write(message)

    def flush(self):
        self.stdout.flush()
        self.file.flush()

    def __del__(self):
        # ê°ì²´ê°€ ì†Œë©¸ë  ë•Œ ì›ë˜ stdoutìœ¼ë¡œ ë³µì›í•˜ê³  íŒŒì¼ì„ ë‹«ìŠµë‹ˆë‹¤.
        sys.stdout = self.stdout
        self.file.close()

# --- ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜: Ollama LLM í˜¸ì¶œ ---
def call_ollama_llm(prompt: str, task_type: str, model: str = "gpt-oss:20b") -> str:
    """
    Ollamaë¥¼ í†µí•´ ë¡œì»¬ LLM APIë¥¼ í˜¸ì¶œí•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    """
    print("=" * 50)
    print(f"ğŸ¤– Ollama '{model}' ëª¨ë¸ì—ê²Œ '{task_type}' ì‘ì—…ì„ ìš”ì²­í•©ë‹ˆë‹¤...")
    print("-" * 50)
    print(f"\n")
    try:
        url = "http://localhost:11434/api/generate"
        payload = {"model": model, "prompt": prompt, "stream": False}
        response = requests.post(url, data=json.dumps(payload))
        response.raise_for_status()
        response_data = response.json()
        generated_text = response_data.get("response", "").strip()
        print(f"âœ… ì‘ì—… ì™„ë£Œ!")
        return generated_text
    except requests.exceptions.RequestException as e:
        print(f"âŒ Ollama API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        print("Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€, í¬íŠ¸ ë²ˆí˜¸(11434)ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return f"ì˜¤ë¥˜: {e}"

# --- ëª¨ë“ˆ 2: CoVe ê²€ì¦ê¸° ---
class ChainOfVerifier:
    """
    CoVe ë…¼ë¬¸ì˜ ê²€ì¦ ë©”ì»¤ë‹ˆì¦˜ì„ êµ¬í˜„í•œ í´ë˜ìŠ¤.
    """
    def __init__(self, model_name: str):
        self.model_name = model_name
    
    # CoVeì˜ ê° ë‹¨ê³„ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜ë“¤...
    def _get_plan_verifications_prompt(self, context_to_verify: str) -> str:
        return f"""You are a fact-checker. Your task is to analyze the provided text and generate a list of simple, verifiable questions to check the factual claims within it. Each question should focus on a single, specific fact.\n\n[Text to Verify]\n{context_to_verify}\n\n[Verification Questions]\n"""
    
    def _get_execute_verification_prompt(self, question: str, source_context: str) -> str:
        return f"""You are an AI assistant. Please answer the following question based ONLY on the provided [Source Context]. Provide a concise and direct answer. If the answer is not in the context, say "I don't know".\n\n[Source Context]\n{source_context}\n\n[Question]\n{question}\n\n[Answer]\n"""

    def _get_final_verification_prompt(self, original_text: str, verifications: dict) -> str:
        verification_log = "\n".join([f"- Q: {q}\n- A: {a}" for q, a in verifications.items()])
        return f"""You are a revision assistant. You will be given an original text and a list of verification question-answer pairs. Your task is to revise the original text to ensure it is consistent with the verification results.\n - If a fact in the original text is contradicted by a verification answer, correct it.\n - If the original text is consistent with the verification, keep it as is.\n - Do not add new information that is not supported by the original text or the verifications. \n\n[Original Text]\n{original_text}\n\n[Verification Log]\n{verification_log}\n\n[Revised Text]\n"""

    def verify(self, text_to_verify: str, source_context: str) -> str:
        """
        ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì— ëŒ€í•´ ì „ì²´ CoVe íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        """
        print("\n--- ğŸ›¡ï¸ CoVe ê²€ì¦ ì‹œì‘ ---")
        # 1. ê²€ì¦ ê³„íš
        plan_prompt = self._get_plan_verifications_prompt(text_to_verify)
        questions_text = call_ollama_llm(plan_prompt, "Plan Verifications", self.model_name)
        questions = re.findall(r'^\d+\.\s*(.*)', questions_text, re.MULTILINE)
        
        if not questions:
            print("âš ï¸ ê²€ì¦ ì§ˆë¬¸ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì›ë³¸ ë©”ëª¨ë¦¬ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return text_to_verify
        print(f"ìƒì„±ëœ ê²€ì¦ ì§ˆë¬¸: {questions}\n")

        # 2. ê²€ì¦ ì‹¤í–‰
        verifications = {}
        for q in questions:
            answer_prompt = self._get_execute_verification_prompt(q, source_context)
            answer = call_ollama_llm(answer_prompt, f"Execute Verification", self.model_name)
            verifications[q] = answer
        print(f"ê²€ì¦ Q&A ê²°ê³¼:\n{json.dumps(verifications, indent=2, ensure_ascii=False)}\n")

        # 3. ìµœì¢… ê²€ì¦ëœ í…ìŠ¤íŠ¸ ìƒì„±
        final_prompt = self._get_final_verification_prompt(text_to_verify, verifications)
        verified_text = call_ollama_llm(final_prompt, "Final Verification", self.model_name)
        print("--- ğŸ›¡ï¸ CoVe ê²€ì¦ ì¢…ë£Œ ---\n")
        return verified_text

# --- ëª¨ë“ˆ 1 + 2: í†µí•©ëœ ê²€ì¦ ê¸°ë°˜ ì¬ê·€ ìš”ì•½ê¸° ---
class VerifiedRecursiveSummarizer:
    """
    R-Sumì˜ ì¬ê·€ì  ìš”ì•½ì— CoVe ê²€ì¦ì„ í†µí•©í•œ ìµœì¢… íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤.
    """
    def __init__(self, model_name: str):
        self.memory = "none"
        self.model_name = model_name
        # CoVe ê²€ì¦ê¸°ë¥¼ ë‚´ë¶€ì ìœ¼ë¡œ ì¸ìŠ¤í„´ìŠ¤í™”
        self.verifier = ChainOfVerifier(model_name)
        print(f"ğŸš€ ê²€ì¦ ê¸°ëŠ¥ì´ íƒ‘ì¬ëœ ì¬ê·€ ìš”ì•½ê¸°ê°€ '{model_name}' ëª¨ë¸ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\n")

    def _get_memory_iteration_prompt(self, session_context: str) -> str:
        return f"""
You are an advanced AI language model with the ability to store and update a memory to keep track of key personality information for both the user and the bot. You will receive a previous memory and dialogue context. Your goal is to update the memory by incorporating the new personality information.
To successfully update the memory, follow these steps:
1. Carefully analyze the existing memory and extract the key personality of the user and bot from it.
2. Consider the dialogue context provided to identify any new or changed personality that needs to be incorporated into the memory.
3. Combine the old and new personality information to create an updated representation of the user and bot's traits.
4. Structure the updated memory in a clear and concise manner, ensuring it does not exceed 20 sentences.
Remember, the memory should serve as a reference point to maintain continuity in the dialogue and help you respond accurately to the user based on their personality.
5. Update your memory in Korean.

[Previous Memory]
{self.memory}

[Session Context]
{session_context}

[Updated Memory]
"""

    def _get_response_generation_prompt(self, current_context: str, last_session_context: str) -> str:
        return f"""
You will be provided with a memory containing personality information for both yourself and the user. Your goal is to respond accurately to the user based on the personality traits and dialogue context.
Follow these steps to successfully complete the task:
1. Analyze the provided memory to extract the key personality traits for both yourself and the user.
2. Review the dialogue history to understand the context and flow of the conversation.
3. Utilize the extracted personality traits and dialogue context to formulate an appropriate response.
4. If no specific personality trait is applicable, respond naturally as a human would.
5. Pay attention to the relevance and importance of the personality information, focusing on capturing the most significant aspects while maintaining the overall coherence of the memory.

[Latest Memory]
{self.memory}

[Last Session Dialogue]
{last_session_context}

[Current Context]
{current_context}

### CRITICAL RULE ###
BEFORE GENERATING A RESPONSE, YOU MUST CHECK THE [Last Session Dialogue]. Answer appropriately based on the conversation from the last session.

[Response]
"""

    def process_dialogue(self, past_sessions: list[str], current_context: str) -> str:
        """
        ì „ì²´ ëŒ€í™” ì„¸ì…˜ì„ ì²˜ë¦¬í•˜ì—¬ ìµœì¢… ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
        """
        print("ğŸ§  ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤: ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸ ë° ê²€ì¦")
        
        # --- ëˆ„ì ëœ ëŒ€í™” ë‚´ìš©ì„ ì €ì¥í•  ë³€ìˆ˜ ---
        accumulated_context = ""
        
        for i, session in enumerate(past_sessions, 1):
            print(f"\n{'='*20} Session {i} ì²˜ë¦¬ ì¤‘ {'='*20}")
            
            # --- í˜„ì¬ ì„¸ì…˜ ë‚´ìš©ì„ ëˆ„ì  ---
            accumulated_context += session + "\n"

            # 1. R-Sum: ë©”ëª¨ë¦¬ ì´ˆì•ˆ ìƒì„±
            memory_prompt = self._get_memory_iteration_prompt(session)
            draft_memory = call_ollama_llm(memory_prompt, f"Memory Draft Generation (S{i})", self.model_name)
            print(f"ğŸ“ ìƒì„±ëœ ë©”ëª¨ë¦¬ ì´ˆì•ˆ (M_{i}_draft):\n{draft_memory}")
            
            # 2. CoVe: ìƒì„±ëœ ë©”ëª¨ë¦¬ ì´ˆì•ˆì„ ì¦‰ì‹œ ê²€ì¦
            # --- source_contextë¡œ 'ëˆ„ì ëœ' ëŒ€í™”ë¥¼ ì „ë‹¬ ---
            verified_memory = self.verifier.verify(
                text_to_verify=draft_memory,
                source_context=accumulated_context
            )
            
            self.memory = verified_memory
            print(f"âœ¨ ìµœì¢… ê²€ì¦ëœ ë©”ëª¨ë¦¬ (M_{i}_verified):\n{self.memory}")
        
        print("\n\nğŸ—£ï¸ ìµœì¢… ì‘ë‹µ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        last_session = past_sessions[-1] if past_sessions else "" # ë§ˆì§€ë§‰ ì„¸ì…˜ ë‚´ìš© ì¶”ì¶œ
        response_prompt = self._get_response_generation_prompt(current_context = current_context, last_session_context=last_session)
        final_response = call_ollama_llm(response_prompt, "Final Response Generation", self.model_name)
        
        return final_response

def load_and_prepare_data(file_path: list[str]) -> tuple[list[str], str]:
    """
    ì—¬ëŸ¬ê°œì˜ JSON ë°ì´í„°ì…‹ íŒŒì¼ì„ ë¡œë“œí•˜ê³  ìŠ¤í¬ë¦½íŠ¸ì— ë§ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    all_past_sessions = [] # ëª¨ë“  ì„¸ì…˜ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    final_current_context = "" # ìµœì¢… current_contextë¥¼ ì €ì¥í•  ë³€ìˆ˜ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

    # ì…ë ¥ë°›ì€ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒí•©ë‹ˆë‹¤.
    for i, file_path in enumerate(file_paths):
        print(f"ğŸ“„ ë°ì´í„° íŒŒì¼ ë¡œë”© ì¤‘: {file_path}")
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")

        # JSON íŒŒì¼ì„ ì—´ê³  ë°ì´í„°ë¥¼ ì½ì–´ì˜µë‹ˆë‹¤.
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        # 'sessions' í‚¤ì— ìˆëŠ” ê° ì„¸ì…˜(ë°œí™” ë¦¬ìŠ¤íŠ¸)ì„ í•˜ë‚˜ì˜ ê¸´ ë¬¸ìì—´ë¡œ í•©ì³ì„œ ì „ì²´ ì„¸ì…˜ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.
        past_sessions = ["\n".join(session) for session in data["sessions"]]
        all_past_sessions.extend(past_sessions)
        if i == len(file_paths) - 1:
            # 'current_context' ë„ ë§ˆì°¬ê°€ì§€ë¡œ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹˜ê³ , ë´‡ì´ ë‹µë³€í•  ì°¨ë¡€ì„ì„ ë‚˜íƒ€ë‚´ëŠ” "Assistant: "ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
            final_current_context = "\n".join(data["current_context"]) + "\nAssistant: "
    if not final_current_context:
        # íŒŒì¼ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆê±°ë‚˜ ë§ˆì§€ë§‰ íŒŒì¼ì— current_contextê°€ ì—†ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ì˜ˆì™¸ ì²˜ë¦¬
        raise ValueError("current_contextë¥¼ ì„¤ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ ë°ì´í„° íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
    return all_past_sessions, final_current_context

# --- ìµœì¢… ì‹¤í–‰ ---
if __name__ == "__main__":
    # <<< ë¡œê·¸ íŒŒì¼ ì„¤ì • ë° í‘œì¤€ ì¶œë ¥ ë¦¬ë””ë ‰ì…˜ >>>
    # ì‹¤í–‰ ì‹œì ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³ ìœ í•œ ë¡œê·¸ íŒŒì¼ ì´ë¦„ ìƒì„±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"Rsum_CoVe_ì‹¤í—˜êµ°_log_{timestamp}.txt"
    
    # Tee í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  print ì¶œë ¥ì„ íŒŒì¼ë¡œë„ ë³´ëƒ„
    sys.stdout = Tee(log_filename)
    
    OLLAMA_MODEL = "gpt-oss:20b"
    file_paths = ["interactive-35.json", "interactive-68.json", "interactive-172.json","interactive-274.json","interactive-283.json","interactive-322.json","interactive-466.json"]

    try:
        # ë°ì´í„°ì…‹ íŒŒì¼ì„ ë¡œë“œí•˜ê³  ì²˜ë¦¬í•©ë‹ˆë‹¤.
        past_sessions, current_context = load_and_prepare_data(file_paths)

        print(f"\nì´ {len(past_sessions)}ê°œì˜ ì„¸ì…˜ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
        
        # í†µí•© í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤í™” ë° ì‹¤í–‰
        summarizer = VerifiedRecursiveSummarizer(model_name=OLLAMA_MODEL)
        final_response = summarizer.process_dialogue(
            past_sessions=past_sessions,
            current_context=current_context
        )
        
        print("\n" + "="*50)
        print("âœ¨ ìµœì¢… ìƒì„±ëœ ë´‡ì˜ ì‘ë‹µ âœ¨")
        print("="*50)
        print(current_context, end="")
        print(final_response)

    except (FileNotFoundError, ValueError) as e:
        print(f"ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    except Exception as e:
        # ì‹¤í–‰ ì¤‘ ì–´ë–¤ ì˜¤ë¥˜ë¼ë„ ë°œìƒí•˜ë©´ ì—¬ê¸°ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
