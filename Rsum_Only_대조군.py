# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
import requests  # HTTP ìš”ì²­ì„ ë³´ë‚´ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ (Ollama API í˜¸ì¶œì— ì‚¬ìš©)
import json      # JSON ë°ì´í„°ë¥¼ ë‹¤ë£¨ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import re        # ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì—ì„œ íŒ¨í„´ì„ ì°¾ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import os        # ìš´ì˜ ì²´ì œì™€ ìƒí˜¸ ì‘ìš©í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ (íŒŒì¼ ê²½ë¡œ í™•ì¸ ë“±)

# --- ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜: Ollama LLM í˜¸ì¶œ (ì €ì êµ¬ì¡°ì™€ ë™ì¼í•˜ê²Œ ìˆ˜ì •) ---
def call_ollama_llm(system_prompt: str, user_prompt: str, task_type: str, model: str = "gpt-oss:20b") -> str:
    """
    Ollamaë¥¼ í†µí•´ ë¡œì»¬ LLM APIë¥¼ í˜¸ì¶œí•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    (ìˆ˜ì •: systemê³¼ user í”„ë¡¬í”„íŠ¸ë¥¼ ë¶„ë¦¬í•˜ì—¬ ì €ìì˜ API í˜¸ì¶œ êµ¬ì¡°ë¥¼ 100% ì¬í˜„)
    """
    # ì‚¬ìš©ìì—ê²Œ í˜„ì¬ ì§„í–‰ ìƒí™©ì„ ì•Œë ¤ì£¼ê¸° ìœ„í•œ ë¡œê·¸ ì¶œë ¥
    print("=" * 50)
    print(f"ğŸ¤– Ollama '{model}' ëª¨ë¸ì—ê²Œ '{task_type}' ì‘ì—…ì„ ìš”ì²­í•©ë‹ˆë‹¤...")
    print("-" * 50)
    print(f"\n[SYSTEM PROMPT]\n{system_prompt}\n")
    print(f"\n[USER PROMPT]\n{user_prompt}\n")
    
    try:
        # Ollama ì„œë²„ì˜ ê¸°ë³¸ API ì£¼ì†Œ
        url = "http://localhost:11434/api/chat" # (ìˆ˜ì •: /api/generate -> /api/chat)
        
        # (ìˆ˜ì •: ì €ìì˜ êµ¬ì¡°ëŒ€ë¡œ messages ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©)
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        # (ìˆ˜ì •: payload êµ¬ì¡° ë³€ê²½)
        payload = {"model": model, "messages": messages, "stream": False, "options": {"temperature": 0.0, "seed":0}}
        
        # requests ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•´ Ollama ì„œë²„ì— POST ìš”ì²­ì„ ë³´ëƒ…ë‹ˆë‹¤.
        response = requests.post(url, data=json.dumps(payload))
        # ë§Œì•½ HTTP ì—ëŸ¬(ì˜ˆ: 404, 500)ê°€ ë°œìƒí•˜ë©´ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤.
        response.raise_for_status()
        
        # ì‘ë‹µ ë°›ì€ JSON ë°ì´í„°ë¥¼ íŒŒì´ì¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        response_data = response.json()
        
        # (ìˆ˜ì •: /api/chat ì‘ë‹µ êµ¬ì¡°ì— ë§ê²Œ íŒŒì‹±)
        generated_text = response_data.get("message", {}).get("content", "").strip()
        
        print(f"âœ… ì‘ì—… ì™„ë£Œ!")
        # ìƒì„±ëœ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        return generated_text
    except requests.exceptions.RequestException as e:
        # API í˜¸ì¶œ ì¤‘ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë“±ì´ ë°œìƒí•˜ë©´ ì˜ˆì™¸ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        print(f"âŒ Ollama API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        print("Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€, í¬íŠ¸ ë²ˆí˜¸(11434)ê°€ ì˜¬ë°”ë¥¸ì§€, /api/chat ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì§€ì›í•˜ëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return f"ì˜¤ë¥˜: {e}"

# --- ëŒ€ì¡°êµ°: R-Sum Only íŒŒì´í”„ë¼ì¸ ---
class RecursiveSummarizer:
    """
    CoVe ê²€ì¦ì´ ì—†ëŠ” ì˜¤ë¦¬ì§€ë„ R-Sum ì¬ê·€ì  ìš”ì•½ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    ì´ê²ƒì´ ì‹¤í—˜ì˜ ëŒ€ì¡°êµ° ì—­í• ì„ í•©ë‹ˆë‹¤.
    """
    def __init__(self, model_name: str):
        self.memory = "none"  # ì´ˆê¸° ë©”ëª¨ë¦¬ëŠ” ë¹„ì–´ìˆëŠ” ìƒíƒœ("none")ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.
        self.model_name = model_name
        print(f"ğŸ§‘â€ğŸ’» ëŒ€ì¡°êµ°(R-Sum Only) ìš”ì•½ê¸°ê°€ '{model_name}' ëª¨ë¸ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\n")

    def _get_memory_iteration_prompt(self, session_context: str) -> tuple[str, str]:
        # (ìˆ˜ì •: ì €ìì˜ í”„ë¡¬í”„íŠ¸ êµ¬ì¡°(system/user)ì— ë§ê²Œ ë°˜í™˜)
        
        # 1. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì €ìì˜ chatgpt/robot.py ì°¸ì¡°)
        system_prompt = "You are an advanced AI language model with the ability to keep track of dialog information between speakers."
        
        # 2. ìœ ì € í”„ë¡¬í”„íŠ¸ (ì €ìì˜ main_chatgpt.py ë° llama/data_loader.py êµ¬ì¡° ì°¸ì¡°)
        # (ì°¸ê³ : ì €ìëŠ” Carecall/MSC ëª¨ë‘ 'user'ì™€ 'system'ìœ¼ë¡œ ê°•ì œ ë§¤í•‘í•˜ì—¬ ìš”ì•½)
        instruction = "You are an advanced AI language model with the ability to store and update a memory to keep track of key personality information for both the user and the system. You will receive a previous memory and a dialogue context. Your goal is to update the memory by incorporating the new personality information while ensuring that the memory does not exceed 20 sentences."
        
        user_prompt = f"""**Instruction** {instruction}

**Test** [Previous Memory] {self.memory} [Dialogue Context] {session_context} [Updated Memory]"""
        
        return system_prompt, user_prompt

    def _get_response_generation_prompt(self, current_context: str, last_session_context: str) -> tuple[str, str]:
        # (ìˆ˜ì •: ì €ìì˜ í”„ë¡¬í”„íŠ¸ êµ¬ì¡°(system/user)ì— ë§ê²Œ ë°˜í™˜)
        
        # 1. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì €ìì˜ chatgpt/robot.py ì°¸ì¡°)
        system_prompt = "You are an advanced AI language model designed to engage in personality-based conversations."
        
        # 2. ìœ ì € í”„ë¡¬í”„íŠ¸ (ì €ìì˜ dataset.py RSumDataset ì°¸ì¡°)
        # (ì°¸ê³ : ì €ìëŠ” LLMì„ 'Assistant'ë¡œ ëª…ëª…í•˜ê³  'User'ì—ê²Œ ì‘ë‹µí•˜ë„ë¡ ì§€ì‹œ)
        instruction = "You are an advanced AI designed for engaging in natural, personality-based conversations. You will be provided with a memory, containing the personal preferences and experiences of speakers (the assistant and the user), as well as a dialogue context. When responding, consider maintaining a conversational and fluent tone. Responses should be contextually relevant, consistent with given memory, aiming to keep the conversation flowing. Human queries are labeled 'User:', while your replies are marked 'Assistant:'. Your goal is to provide engaging and coherent responses based on the dialogue context provided."
        
        # (ì°¸ê³ : ì €ìì˜ í”„ë¡¬í”„íŠ¸ëŠ” [Last Session Dialogue]ë¥¼ ëª…ì‹œì ìœ¼ë¡œ í¬í•¨í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 
        # MSC ë°ì´í„°ë¡œë”ëŠ” 'window'ì— í˜„ì¬ ì„¸ì…˜ì˜ ëŒ€í™”ë§Œ í¬í•¨í•©ë‹ˆë‹¤.)
        user_prompt = f"""**Instruction** {instruction}

**Test** [Previous Memory] {self.memory} [Dialogue Context] {current_context} [Response] 
"""
        # (ì°¸ê³ : [Response] ë’¤ì— 'Assistant:'ë¥¼ ë¶™ì´ì§€ ì•Šì•„ë„, 
        # current_contextì˜ ë§ˆì§€ë§‰ì´ 'Assistant: 'ë¡œ ëë‚˜ë¯€ë¡œ ëª¨ë¸ì´ ì´ì–´ì„œ ìƒì„±í•˜ê²Œ ë©ë‹ˆë‹¤.)
        
        return system_prompt, user_prompt

    def process_dialogue(self, past_sessions: list[str], current_context: str) -> str:
        """
        ì „ì²´ ëŒ€í™” ì„¸ì…˜ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ìµœì¢… ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
        (ìˆ˜ì •: call_ollama_llm ì¸ì êµ¬ì¡° ë³€ê²½)
        """
        print("ğŸ§  ëŒ€ì¡°êµ° í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤: ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸ Only")
        
        # ê³¼ê±° ëŒ€í™” ì„¸ì…˜ë“¤ì„ í•˜ë‚˜ì”© ìˆœíšŒí•©ë‹ˆë‹¤.
        for i, session in enumerate(past_sessions, 1):
            print(f"\n{'='*20} Session {i} ì²˜ë¦¬ ì¤‘ {'='*20}")
            
            # 1. R-Sum: í˜„ì¬ ì„¸ì…˜ ëŒ€í™”ì™€ ì´ì „ ë©”ëª¨ë¦¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë©”ëª¨ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
            system_prompt, user_prompt = self._get_memory_iteration_prompt(session)
            new_memory = call_ollama_llm(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                task_type=f"Memory Generation (S{i})", 
                model=self.model_name
            )
            
            # 2. CoVe ê²€ì¦ ì—†ì´ ìƒì„±ëœ ë©”ëª¨ë¦¬ë¥¼ ë°”ë¡œ ê³µì‹ ë©”ëª¨ë¦¬ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
            self.memory = new_memory
            print(f"ğŸ“ ìƒì„± ë° í™•ì •ëœ ë©”ëª¨ë¦¬ (M_{i}):\n{self.memory}")
        
        # ëª¨ë“  ê³¼ê±° ì„¸ì…˜ ì²˜ë¦¬ê°€ ëë‚˜ë©´, ìµœì¢… ë©”ëª¨ë¦¬ë¥¼ ì´ìš©í•´ ì‚¬ìš©ìì—ê²Œ ì‘ë‹µí•  ì°¨ë¡€ì…ë‹ˆë‹¤.
        print("\n\nğŸ—£ï¸ ìµœì¢… ì‘ë‹µ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        last_session = past_sessions[-1] if past_sessions else "" # ë§ˆì§€ë§‰ ë‚´ìš© ì¶”ì¶œ
        
        system_prompt, user_prompt = self._get_response_generation_prompt(
            current_context=current_context, 
            last_session_context=last_session
        )
        final_response = call_ollama_llm(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            task_type="Final Response Generation", 
            model=self.model_name
        )
        
        return final_response

def load_and_prepare_data(file_path: list[str]) -> tuple[list[str], str]:
    """
    ì—¬ëŸ¬ê°œì˜ JSON ë°ì´í„°ì…‹ íŒŒì¼ì„ ë¡œë“œí•˜ê³  ìŠ¤í¬ë¦½íŠ¸ì— ë§ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    all_past_sessions = [] # ëª¨ë“  ì„¸ì…˜ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    final_current_context = "" # ìµœì¢… current_contextë¥¼ ì €ì¥í•  ë³€ìˆ˜ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

    # ì…ë ¥ë°›ì€ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒí•©ë‹ˆë‹¤.
    for i, file_path in enumerate(file_paths):
        print(f"ğŸ“„ ë°ì´í„° íŒŒì¼ ë¡œë”© ì¤‘: {file_path}")
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")

        # JSON íŒŒì¼ì„ ì—´ê³  ë°ì´í„°ë¥¼ ì½ì–´ì˜µë‹ˆë‹¤.
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        # 'sessions' í‚¤ì— ìˆëŠ” ê° ì„¸ì…˜(ë°œí™” ë¦¬ìŠ¤íŠ¸)ì„ í•˜ë‚˜ì˜ ê¸´ ë¬¸ìì—´ë¡œ í•©ì³ì„œ ì „ì²´ ì„¸ì…˜ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.
        past_sessions = ["\n".join(session) for session in data["sessions"]]
        all_past_sessions.extend(past_sessions)
        if i == len(file_paths) - 1:
            # 'current_context' ë„ ë§ˆì°¬ê°€ì§€ë¡œ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹˜ê³ , ë´‡ì´ ë‹µë³€í•  ì°¨ë¡€ì„ì„ ë‚˜íƒ€ë‚´ëŠ” "Assistant: "ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
            final_current_context = "\n".join(data["current_context"]) + "\nAssistant: "

    if not final_current_context:
        # íŒŒì¼ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆê±°ë‚˜, ë§ˆì§€ë§‰ íŒŒì¼ì— current_contextê°€ ì—†ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ì˜ˆì™¸ ì²˜ë¦¬
        raise ValueError("current_contextë¥¼ ì„¤ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ ë°ì´í„° íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
    return all_past_sessions, final_current_context

# --- ìµœì¢… ì‹¤í–‰ ---
# ì´ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì´ ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ ì•„ë˜ ì½”ë“œê°€ ë™ì‘í•©ë‹ˆë‹¤.
if __name__ == "__main__":
    OLLAMA_MODEL = "gpt-oss:20b"
    file_paths = ["interactive-35.json", "interactive-68.json", "interactive-172.json","interactive-274.json","interactive-283.json","interactive-322.json","interactive-466.json"]

    try:
        # ë°ì´í„°ì…‹ íŒŒì¼ì„ ë¡œë“œí•˜ê³  ì²˜ë¦¬í•©ë‹ˆë‹¤.
        past_sessions, current_context = load_and_prepare_data(file_paths)

        print(f"\nì´ {len(past_sessions)}ê°œì˜ ì„¸ì…˜ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
        
        # ëŒ€ì¡°êµ° í´ë˜ìŠ¤ì¸ RecursiveSummarizerë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        summarizer = RecursiveSummarizer(model_name=OLLAMA_MODEL)
        # ì „ì²´ ëŒ€í™” ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰í•˜ì—¬ ìµœì¢… ì‘ë‹µì„ ì–»ìŠµë‹ˆë‹¤.
        final_response = summarizer.process_dialogue(
            past_sessions=past_sessions,
            current_context=current_context
        )
        
        # ìµœì¢… ê²°ê³¼ë¥¼ ì˜ˆì˜ê²Œ ì¶œë ¥í•©ë‹ˆë‹¤.
        print("\n" + "="*50)
        print("âœ¨ ìµœì¢… ìƒì„±ëœ ë´‡ì˜ ì‘ë‹µ (ëŒ€ì¡°êµ°) âœ¨")
        print("="*50)
        print(current_context, end="")
        print(final_response)
        
    except (FileNotFoundError, ValueError) as e:
        print(f"ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    except Exception as e:
        # ì‹¤í–‰ ì¤‘ ì–´ë–¤ ì˜¤ë¥˜ë¼ë„ ë°œìƒí•˜ë©´ ì—¬ê¸°ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")



