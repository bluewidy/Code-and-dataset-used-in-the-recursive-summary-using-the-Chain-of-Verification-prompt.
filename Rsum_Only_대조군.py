# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
import requests  # HTTP ìš”ì²­ì„ ë³´ë‚´ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ (Ollama API í˜¸ì¶œì— ì‚¬ìš©)
import json      # JSON ë°ì´í„°ë¥¼ ë‹¤ë£¨ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import re        # ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì—ì„œ íŒ¨í„´ì„ ì°¾ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import os        # ìš´ì˜ ì²´ì œì™€ ìƒí˜¸ ì‘ìš©í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ (íŒŒì¼ ê²½ë¡œ í™•ì¸ ë“±)

# --- ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜: Ollama LLM í˜¸ì¶œ ---
def call_ollama_llm(prompt: str, task_type: str, model: str = "gpt-oss:20b") -> str:
    """
    Ollamaë¥¼ í†µí•´ ë¡œì»¬ LLM APIë¥¼ í˜¸ì¶œí•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    """
    # ì‚¬ìš©ìì—ê²Œ í˜„ì¬ ì§„í–‰ ìƒí™©ì„ ì•Œë ¤ì£¼ê¸° ìœ„í•œ ë¡œê·¸ ì¶œë ¥
    print("=" * 50)
    print(f"ğŸ¤– Ollama '{model}' ëª¨ë¸ì—ê²Œ '{task_type}' ì‘ì—…ì„ ìš”ì²­í•©ë‹ˆë‹¤...")
    print("-" * 50)
    print(f"\n")
    try:
        # Ollama ì„œë²„ì˜ ê¸°ë³¸ API ì£¼ì†Œ
        url = "http://localhost:11434/api/generate"
        # APIì— ë³´ë‚¼ ë°ì´í„°. ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ê³¼ í”„ë¡¬í”„íŠ¸ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
        payload = {"model": model, "prompt": prompt, "stream": False}
        # requests ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•´ Ollama ì„œë²„ì— POST ìš”ì²­ì„ ë³´ëƒ…ë‹ˆë‹¤.
        response = requests.post(url, data=json.dumps(payload))
        # ë§Œì•½ HTTP ì—ëŸ¬(ì˜ˆ: 404, 500)ê°€ ë°œìƒí•˜ë©´ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤.
        response.raise_for_status()
        # ì‘ë‹µ ë°›ì€ JSON ë°ì´í„°ë¥¼ íŒŒì´ì¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        response_data = response.json()
        # ë”•ì…”ë„ˆë¦¬ì—ì„œ 'response' í‚¤ì— í•´ë‹¹í•˜ëŠ” ê°’(LLMì´ ìƒì„±í•œ í…ìŠ¤íŠ¸)ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        generated_text = response_data.get("response", "").strip()
        print(f"âœ… ì‘ì—… ì™„ë£Œ!")
        # ìƒì„±ëœ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        return generated_text
    except requests.exceptions.RequestException as e:
        # API í˜¸ì¶œ ì¤‘ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë“±ì´ ë°œìƒí•˜ë©´ ì˜ˆì™¸ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        print(f"âŒ Ollama API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        print("Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€, í¬íŠ¸ ë²ˆí˜¸(11434)ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return f"ì˜¤ë¥˜: {e}"

# --- ëŒ€ì¡°êµ°: R-Sum Only íŒŒì´í”„ë¼ì¸ ---
class RecursiveSummarizer:
    """
    CoVe ê²€ì¦ì´ ì—†ëŠ” ì˜¤ë¦¬ì§€ë„ R-Sum ì¬ê·€ì  ìš”ì•½ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    ì´ê²ƒì´ ì‹¤í—˜ì˜ ëŒ€ì¡°êµ° ì—­í• ì„ í•©ë‹ˆë‹¤.
    """
    def __init__(self, model_name: str):
        self.memory = "none"  # ì´ˆê¸° ë©”ëª¨ë¦¬ëŠ” ë¹„ì–´ìˆëŠ” ìƒíƒœ("none")ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.
        self.model_name = model_name
        print(f"ğŸ§‘â€ğŸ’» ëŒ€ì¡°êµ°(R-Sum Only) ìš”ì•½ê¸°ê°€ '{model_name}' ëª¨ë¸ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\n")

    def _get_memory_iteration_prompt(self, session_context: str) -> str:
        # R-Sum ë‹¨ê³„: ì´ì „ ë©”ëª¨ë¦¬ì™€ í˜„ì¬ ì„¸ì…˜ ëŒ€í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ ë©”ëª¨ë¦¬ë¥¼ ë§Œë“¤ë„ë¡ ì§€ì‹œí•˜ëŠ” í”„ë¡¬í”„íŠ¸ì…ë‹ˆë‹¤.
        return f"""
You are an advanced AI language model with the ability to store and update a memory to keep track of key personality information for both the user and the bot. You will receive a previous memory and dialogue context. Your goal is to update the memory by incorporating the new personality information.
To successfully update the memory, follow these steps:
1. Carefully analyze the existing memory and extract the key personality of the user and bot from it.
2. Consider the dialogue context provided to identify any new or changed personality that needs to be incorporated into the memory.
3. Combine the old and new personality information to create an updated representation of the user and bot's traits.
4. Structure the updated memory in a clear and concise manner, ensuring it does not exceed 20 sentences.
Remember, the memory should serve as a reference point to maintain continuity in the dialogue and help you respond accurately to the user based on their personality.
5. Update your memory in Korean.

[Previous Memory]
{self.memory}

[Session Context]
{session_context}

[Updated Memory]
"""

    def _get_response_generation_prompt(self, current_context: str, last_session_context: str) -> str:
        # ìµœì¢… ì‘ë‹µ ìƒì„± ë‹¨ê³„: ìµœì‹  ë©”ëª¨ë¦¬ì™€ í˜„ì¬ ëŒ€í™” ë¬¸ë§¥ì„ ë³´ê³  ì¼ê´€ì„± ìˆëŠ” ë‹µë³€ì„ ìƒì„±í•˜ë„ë¡ ì§€ì‹œí•˜ëŠ” í”„ë¡¬í”„íŠ¸ì…ë‹ˆë‹¤.
        return f"""
You will be provided with a memory containing personality information for both yourself and the user. Your goal is to respond accurately to the user based on the personality traits and dialogue context.
Follow these steps to successfully complete the task:
1. Analyze the provided memory to extract the key personality traits for both yourself and the user.
2. Review the dialogue history to understand the context and flow of the conversation.
3. Utilize the extracted personality traits and dialogue context to formulate an appropriate response.
4. If no specific personality trait is applicable, respond naturally as a human would.
5. Pay attention to the relevance and importance of the personality information, focusing on capturing the most significant aspects while maintaining the overall coherence of the memory.

[Latest Memory]
{self.memory}

[Last Session Dialogue]
{last_session_context}

[Current Context]
{current_context}

### CRITICAL RULE ###
BEFORE GENERATING A RESPONSE, YOU MUST CHECK THE [Last Session Dialogue]. Answer appropriately based on the conversation from the last session.

[Response]
"""

    def process_dialogue(self, past_sessions: list[str], current_context: str) -> str:
        """
        ì „ì²´ ëŒ€í™” ì„¸ì…˜ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ìµœì¢… ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
        """
        print("ğŸ§  ëŒ€ì¡°êµ° í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤: ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸ Only")
        
        # ê³¼ê±° ëŒ€í™” ì„¸ì…˜ë“¤ì„ í•˜ë‚˜ì”© ìˆœíšŒí•©ë‹ˆë‹¤.
        for i, session in enumerate(past_sessions, 1):
            print(f"\n{'='*20} Session {i} ì²˜ë¦¬ ì¤‘ {'='*20}")
            
            # 1. R-Sum: í˜„ì¬ ì„¸ì…˜ ëŒ€í™”ì™€ ì´ì „ ë©”ëª¨ë¦¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë©”ëª¨ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
            memory_prompt = self._get_memory_iteration_prompt(session)
            new_memory = call_ollama_llm(memory_prompt, f"Memory Generation (S{i})", self.model_name)
            
            # 2. CoVe ê²€ì¦ ì—†ì´ ìƒì„±ëœ ë©”ëª¨ë¦¬ë¥¼ ë°”ë¡œ ê³µì‹ ë©”ëª¨ë¦¬ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
            self.memory = new_memory
            print(f"ğŸ“ ìƒì„± ë° í™•ì •ëœ ë©”ëª¨ë¦¬ (M_{i}):\n{self.memory}")
        
        # ëª¨ë“  ê³¼ê±° ì„¸ì…˜ ì²˜ë¦¬ê°€ ëë‚˜ë©´, ìµœì¢… ë©”ëª¨ë¦¬ë¥¼ ì´ìš©í•´ ì‚¬ìš©ìì—ê²Œ ì‘ë‹µí•  ì°¨ë¡€ì…ë‹ˆë‹¤.
        print("\n\nğŸ—£ï¸ ìµœì¢… ì‘ë‹µ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        last_session = past_sessions[-1] if past_sessions else "" # ë§ˆì§€ë§‰ ë‚´ìš© ì¶”ì¶œ
        response_prompt = self._get_response_generation_prompt(current_context = current_context, last_session_context = last_session)
        final_response = call_ollama_llm(response_prompt, "Final Response Generation", self.model_name)
        
        return final_response

def load_and_prepare_data(file_path: list[str]) -> tuple[list[str], str]:
    """
    ì—¬ëŸ¬ê°œì˜ JSON ë°ì´í„°ì…‹ íŒŒì¼ì„ ë¡œë“œí•˜ê³  ìŠ¤í¬ë¦½íŠ¸ì— ë§ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    all_past_sessions = [] # ëª¨ë“  ì„¸ì…˜ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    final_current_context = "" # ìµœì¢… current_contextë¥¼ ì €ì¥í•  ë³€ìˆ˜ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

    # ì…ë ¥ë°›ì€ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒí•©ë‹ˆë‹¤.
    for i, file_path in enumerate(file_paths):
        print(f"ğŸ“„ ë°ì´í„° íŒŒì¼ ë¡œë”© ì¤‘: {file_path}")
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")

        # JSON íŒŒì¼ì„ ì—´ê³  ë°ì´í„°ë¥¼ ì½ì–´ì˜µë‹ˆë‹¤.
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        # 'sessions' í‚¤ì— ìˆëŠ” ê° ì„¸ì…˜(ë°œí™” ë¦¬ìŠ¤íŠ¸)ì„ í•˜ë‚˜ì˜ ê¸´ ë¬¸ìì—´ë¡œ í•©ì³ì„œ ì „ì²´ ì„¸ì…˜ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.
        past_sessions = ["\n".join(session) for session in data["sessions"]]
        all_past_sessions.extend(past_sessions)
        if i == len(file_paths) - 1:
            # 'current_context' ë„ ë§ˆì°¬ê°€ì§€ë¡œ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹˜ê³ , ë´‡ì´ ë‹µë³€í•  ì°¨ë¡€ì„ì„ ë‚˜íƒ€ë‚´ëŠ” "Assistant: "ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
            final_current_context = "\n".join(data["current_context"]) + "\nAssistant: "

    if not final_current_context:
        # íŒŒì¼ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆê±°ë‚˜, ë§ˆì§€ë§‰ íŒŒì¼ì— current_contextê°€ ì—†ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ì˜ˆì™¸ ì²˜ë¦¬
        raise ValueError("current_contextë¥¼ ì„¤ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ ë°ì´í„° íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
    return all_past_sessions, final_current_context

# --- ìµœì¢… ì‹¤í–‰ ---
# ì´ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì´ ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ ì•„ë˜ ì½”ë“œê°€ ë™ì‘í•©ë‹ˆë‹¤.
if __name__ == "__main__":
    OLLAMA_MODEL = "gpt-oss:20b"
    file_paths = ["interactive-35.json", "interactive-68.json", "interactive-172.json","interactive-274.json","interactive-283.json","interactive-322.json","interactive-466.json"]

    try:
        # ë°ì´í„°ì…‹ íŒŒì¼ì„ ë¡œë“œí•˜ê³  ì²˜ë¦¬í•©ë‹ˆë‹¤.
        past_sessions, current_context = load_and_prepare_data(file_paths)

        print(f"\nì´ {len(past_sessions)}ê°œì˜ ì„¸ì…˜ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
        
        # ëŒ€ì¡°êµ° í´ë˜ìŠ¤ì¸ RecursiveSummarizerë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        summarizer = RecursiveSummarizer(model_name=OLLAMA_MODEL)
        # ì „ì²´ ëŒ€í™” ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰í•˜ì—¬ ìµœì¢… ì‘ë‹µì„ ì–»ìŠµë‹ˆë‹¤.
        final_response = summarizer.process_dialogue(
            past_sessions=past_sessions,
            current_context=current_context
        )
        
        # ìµœì¢… ê²°ê³¼ë¥¼ ì˜ˆì˜ê²Œ ì¶œë ¥í•©ë‹ˆë‹¤.
        print("\n" + "="*50)
        print("âœ¨ ìµœì¢… ìƒì„±ëœ ë´‡ì˜ ì‘ë‹µ (ëŒ€ì¡°êµ°) âœ¨")
        print("="*50)
        print(current_context, end="")
        print(final_response)
        
    except (FileNotFoundError, ValueError) as e:
        print(f"ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    except Exception as e:
        # ì‹¤í–‰ ì¤‘ ì–´ë–¤ ì˜¤ë¥˜ë¼ë„ ë°œìƒí•˜ë©´ ì—¬ê¸°ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")

