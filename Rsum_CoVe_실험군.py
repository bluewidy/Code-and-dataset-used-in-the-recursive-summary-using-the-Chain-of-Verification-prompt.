import requests
import json
import re
import os
import sys
from datetime import datetime

# <<< Tee í´ë˜ìŠ¤ (ìœ ì§€) >>>
class Tee:
    def __init__(self, filename, mode='w', encoding='utf-8'):
        self.file = open(filename, mode, encoding=encoding)
        self.stdout = sys.stdout
        sys.stdout = self
    def write(self, message):
        self.stdout.write(message)
        self.file.write(message)
    def flush(self):
        self.stdout.flush()
        self.file.flush()
    def __del__(self):
        sys.stdout = self.stdout
        self.file.close()

# --- (ìˆ˜ì •) ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜: Ollama LLM í˜¸ì¶œ (Temperature 0 ì ìš©) ---
def call_ollama_llm(system_prompt: str, user_prompt: str, task_type: str, model: str = "gpt-oss:20b") -> str:
    """
    Ollamaë¥¼ í†µí•´ ë¡œì»¬ LLM APIë¥¼ í˜¸ì¶œí•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    (ìˆ˜ì •: optionsì— temperature=0.0 ì¶”ê°€í•˜ì—¬ ë¬´ì‘ìœ„ì„± ì œê±°)
    """
    print("=" * 50)
    print(f"ğŸ¤– Ollama '{model}' ëª¨ë¸ì—ê²Œ '{task_type}' ì‘ì—…ì„ ìš”ì²­í•©ë‹ˆë‹¤...")
    print("-" * 50)
    print(f"\n[SYSTEM PROMPT]\n{system_prompt}\n")
    print(f"\n[USER PROMPT]\n{user_prompt}\n")
    
    try:
        url = "http://localhost:11434/api/chat"
        payload = {
            "model": model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "stream": False,
            # ğŸ’¡ (í•µì‹¬ ìˆ˜ì •) Temperatureë¥¼ 0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ê²°ê³¼ì˜ ì¼ê´€ì„± ë³´ì¥
            "options": {
                "temperature": 0.0,
                "seed":0
            }
        }
        response = requests.post(url, json=payload, timeout=30000) 
        response.raise_for_status()
        
        response_data = response.json()
        content = response_data['message']['content'].strip()
        
        print(f"âœ… Ollama ì‘ë‹µ ìˆ˜ì‹ :\n{content}")
        return content
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ Ollama API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
        return f"LLM í˜¸ì¶œ ì˜¤ë¥˜: {e}"

# --- ì‹¤í—˜êµ° (Rsum + CoVe) í´ë˜ìŠ¤ ---
class VerifiedRecursiveSummarizer:
    def __init__(self, model_name: str):
        self.model_name = model_name
        
        self.memory_system_prompt = "You are an advanced AI language model with the ability to keep track of dialog information between speakers."
        self.memory_instruction = "You are an advanced AI language model with the ability to store and update a memory to keep track of key personality information for both the user and the system. You will receive a previous memory and a dialogue context. Your goal is to update the memory by incorporating the new personality information while ensuring that the memory does not exceed 20 sentences."
        
        self.response_system_prompt = "You are an advanced AI language model designed to engage in personality-based conversations."
        self.response_instruction = "You are an advanced AI designed for engaging in natural, personality-based conversations. You will be provided with a memory, containing the personal preferences and experiences of speakers (the assistant and the user), as well as a dialogue context. When responding, consider maintaining a conversational and fluent tone. Responses should be contextually relevant, consistent with given memory, aiming to keep the conversation flowing. Human queries are labeled 'User:', while your replies are marked 'Assistant:'. Your goal is to provide engaging and coherent responses based on the dialogue context provided."

    # Step 1: ìš”ì•½ ìƒì„± (ìœ ì§€)
    def _generate_memory(self, prev_memory: str, current_context: str, session_num: int) -> str:
        print("\n" + "---" * 10)
        print(f"ğŸŒ€ (Step 1) ìš”ì•½ ìƒì„± (R-Sum) - S{session_num}")
        print("---" * 10)
        
        memory_context_for_prompt = prev_memory if prev_memory and prev_memory.strip() else "none"
        
        system_prompt = self.memory_system_prompt
        user_prompt = f"""**Instruction** {self.memory_instruction}

**Test** [Previous Memory] {memory_context_for_prompt} [Dialogue Context] {current_context} [Updated Memory]"""
        
        task_name = f"Memory Generation (S{session_num})"
        return call_ollama_llm(system_prompt, user_prompt, task_name, self.model_name)

    # Step 2: Diff ìƒì„± (ìœ ì§€)
    def _get_memory_diff(self, old_memory: str, new_memory: str) -> str:
        print("\n" + "---" * 10)
        print("ğŸ” (Step 2) 'Diff' ìƒì„±: ì´ì „ ë©”ëª¨ë¦¬ì™€ ìƒˆ ìš”ì•½ë³¸ ë¹„êµ ì¤‘...")
        print("---" * 10)
        
        system_prompt = "You are an AI analyzer. Your task is to compare a 'Previous Memory' with a 'New Memory' and extract *only* the facts that are NEW or MODIFIED in the 'New Memory'. Facts that are simply carried over (unchanged) should be ignored."
        user_prompt = f"""
[Previous Memory]
{old_memory}

[New Memory]
{new_memory}

[Instruction]
List all facts that are NEWLY ADDED or SIGNIFICANTLY MODIFIED in the '[New Memory]'.
- If no new or modified facts are found, output the exact string "NO CHANGES".
- Focus only on the delta (the changes).

[New or Modified Facts]
"""
        diff_facts = call_ollama_llm(system_prompt, user_prompt, "Memory Diff Generation", self.model_name)
        
        if "NO CHANGES" in diff_facts.upper() or len(diff_facts) < 5:
            print("â„¹ï¸ (Step 2) 'Diff' ê²°ê³¼: ë³€ê²½ ì‚¬í•­ ì—†ìŒ.")
            return ""
        return diff_facts

    # Step 3: ì§ˆë¬¸ ìƒì„± (ìœ ì§€)
    def _generate_verification_questions(self, facts_to_verify: str) -> list[str]:
        print("\n" + "---" * 10)
        print(f"â“ (Step 3) 'Diff' ê¸°ë°˜ ê²€ì¦ ì§ˆë¬¸ ìƒì„± ì¤‘...")
        print("---" * 10)
        
        system_prompt = "You are an AI assistant. Given a list of facts, generate a set of simple, verifiable questions to check if these facts are true."
        user_prompt = f"""
[Facts to Verify]
{facts_to_verify}

[Instruction]
Generate a list of verification questions based *only* on the facts provided above.
- Each question should be on a new line.
- Do NOT use hyphens or numbers.

[Verification Questions]
"""
        questions_text = call_ollama_llm(system_prompt, user_prompt, "Question Generation (Diff)", self.model_name)
        questions = [q.strip() for q in questions_text.split('\n') if q.strip() and '?' in q]
        
        print(f"âœ… {len(questions)}ê°œì˜ ê²€ì¦ ì§ˆë¬¸ ìƒì„± ì™„ë£Œ.")
        return questions

    # -----------------------------------------------------------------
    # ğŸ’¡ (ìˆ˜ì •) Step 4: ììœ  ì„œìˆ í˜• ê²€ì¦ (True/False ì œì•½ ì‚­ì œ)
    # -----------------------------------------------------------------
    def _execute_verification_plan(self, questions: list[str], 
                                 current_session_context: str) -> dict:
        print("\n" + "---" * 10)
        print(f"ğŸ›¡ï¸ (Step 4) 'ìµœì†Œ ì»¨í…ìŠ¤íŠ¸' ê²€ì¦ ì‹¤í–‰ ì¤‘... (ììœ  ì„œìˆ í˜• ë‹µë³€)")
        print("---" * 10)

        # (ìˆ˜ì •) ë‹¨ë‹µí˜• ì œì•½ ì‚­ì œ ë° ì„¤ëª… ìš”ì²­
        system_prompt = "You are an AI fact-checker. Verify the question based *strictly* on the provided [Context]. Provide exact quotes."
        
        verified_answers = {}
        for q in questions:
            user_prompt = f"""
[Context]
{current_session_context}

[Question]
{q}

[Instruction]
Answer based *strictly* on the context.
1. **Logic Check:** Do not conflate two different people's attributes.
2. **State Check:** Distinguish between past origin and current status.
3. **Nuance Check:** Distinguish between "ability" (can do) and "interest" (likes to watch/read).
4. **No Inference:** Do not assume unstated preferences based on facts.
5. **(CRITICAL) Provide a direct quote.**

[Few-Shot Examples (Domain-Agnostic)]

Example 1 (Origin vs Residence - Logic: Past != Current):
Context: User: "I grew up in Texas, but I've been living in Tokyo for 5 years."
Question: Does the user live in Texas?
Answer: No, the user is *from* Texas but currently *lives* in Tokyo.
Quote: "grew up in Texas... living in Tokyo"

Example 2 (Entity Binding - Logic: Speaker A's action != Speaker B's location):
Context: User: "I'm eating a burger." Assistant: "Delicious! I'm reading a book in the library."
Question: Is the user eating a burger in the library?
Answer: No. The User is eating a burger, but the location "library" applies to the Assistant.
Quote: "User: ...eating a burger", "Assistant: ...in the library"

Example 3 (Ability vs Interest - Logic: Cannot do != Dislike):
Context: Assistant: "I can't play the guitar, but I love listening to rock music."
Question: Is the assistant uninterested in guitars?
Answer: No. She lacks the ability to play (can't play), but she has an interest in the music (loves listening).
Quote: "can't play... love listening"

Example 4 (Fact vs Inference - Logic: Possession != Profession/Hobby):
Context: Assistant: "I own a vintage Ferrari."
Question: Is the assistant a professional racing driver?
Answer: Not mentioned. Owning a car does not imply being a professional driver.
Quote: "own a vintage Ferrari"

Example 5 (Plan vs Fact - Logic: Future != Present):
Context: User: "I plan to study French next year, currently I speak Spanish."
Question: Does the user speak French?
Answer: No, speaking French is a future plan. Currently, they speak Spanish.
Quote: "plan to study... next year"

Example 6 (Selection Nuance - Logic: A or B != A and B):
Context: User: "I'll buy either the red shirt or the blue one."
Question: Is the user buying both shirts?
Answer: No, the user is choosing between the two ("either... or"), not buying both.
Quote: "either the red shirt or the blue one"

Example 7 (External Knowledge - Logic: General != Specific):
Context: User: "I work at a tech company in Silicon Valley."
Question: Does the user work for Google?
Answer: Not mentioned. The context says "a tech company", it does not specify "Google".
Quote: "work at a tech company"

[Answer]
"""
            task_name = f"Fact Verification (Q: {q[:40]}...)"
            answer = call_ollama_llm(system_prompt, user_prompt, task_name, self.model_name)
            verified_answers[q] = answer.strip()
        
        print("âœ… ëª¨ë“  Diff ì§ˆë¬¸ ê²€ì¦ ì™„ë£Œ.")
        return verified_answers

    # -----------------------------------------------------------------
    # ğŸ’¡ (ìˆ˜ì • v3.3) Step 5: ì´ˆì•ˆ êµì • (Draft Correction) & GC
    # -----------------------------------------------------------------
    def _reconstruct_final_memory(self, draft_memory: str, verified_answers: dict) -> str:
        # (ìˆ˜ì •) ì…ë ¥ ë³€ìˆ˜ëª…ì„ old_memory -> draft_memoryë¡œ ë³€ê²½í•˜ì—¬ ì˜ë¯¸ ëª…í™•í™”
        
        qa_pairs_str = ""
        for q, a in verified_answers.items():
            qa_pairs_str += f"Q: {q}\nA: {a}\n\n"

        print("\n" + "---" * 10)
        print("ğŸ“ (Step 5) ë©”ëª¨ë¦¬ êµì • ë° ì¬êµ¬ì„± (Correction & GC)...")
        print("---" * 10)

        # (ìˆ˜ì •) í”„ë¡¬í”„íŠ¸: ë³‘í•©(Merge)ì´ ì•„ë‹ˆë¼ ìˆ˜ì •(Correct/Refine)ì— ì´ˆì 
        system_prompt = "You are an AI memory editor. Your task is to correct and polish the [Draft Memory] based on the [Verification Results]."
        user_prompt = f"""
[Draft Memory]
{draft_memory}

[Verification Results (Fact-Check)]
{qa_pairs_str}

[Instruction]
Refine the [Draft Memory] to create the [Final Verified Memory].
1. **Correction:** If the [Verification Results] contradict any statement in the draft, **rewrite or remove** that statement in the draft. (Trust the Verification Results).
2. **Garbage Collection:** Remove any [PLAN] or [INTENTION] from the draft that is clearly outdated or completed based on the context.
3. **Constraint:** Ensure the final output is a concise list (under 20 sentences).

[Final Verified Memory]
"""
        final_memory = call_ollama_llm(system_prompt, user_prompt, "Final Memory Reconstruction", self.model_name)
        return final_memory

    # -----------------------------------------------------------------
    # ğŸ’¡ ë©”ì¸ ì‹¤í–‰ê¸° (ìˆ˜ì •)
    # -----------------------------------------------------------------
    def process_dialogue(self, past_sessions: list[str], current_context: str) -> str:
        print(f"ğŸ§‘â€ğŸ’» {self.__class__.__name__}ê°€ '{self.model_name}' ëª¨ë¸ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("\nğŸ§  (v3.3: Draft Correction ëª¨ë“œ) í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

        current_verified_summary = "" 
        all_sessions = past_sessions 
        
        if not all_sessions:
            print("âš ï¸ ê²½ê³ : 'past_sessions'ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        else:
            for i, session_context in enumerate(all_sessions):
                session_number = i + 1 
                print(f"\n\n{'='*25} Session {session_number} ì²˜ë¦¬ ì¤‘ {'='*25}")
                
                # Step 1: ìš”ì•½ (R-Sum) -> ì´ˆì•ˆ ìƒì„±
                memory_draft = self._generate_memory(
                    prev_memory=current_verified_summary, 
                    current_context=session_context, 
                    session_num=session_number
                )
                
                # Step 2: Diff -> ì´ˆì•ˆì—ì„œ ìƒˆë¡œìš´ ë¶€ë¶„ ê°ì§€
                memory_diff = self._get_memory_diff(
                    old_memory=current_verified_summary, 
                    new_memory=memory_draft
                )
                
                if not memory_diff.strip():
                    print("â„¹ï¸ (ë©”ì¸ ë£¨í”„) ë³€ê²½ë¶„ ì—†ìŒ. ì´ˆì•ˆì„ ê·¸ëŒ€ë¡œ ì±„íƒí•©ë‹ˆë‹¤.")
                    current_verified_summary = memory_draft 
                    continue 

                # Step 3: ì§ˆë¬¸ ìƒì„±
                questions = self._generate_verification_questions(memory_diff)
                if not questions:
                    print("â„¹ï¸ (ë©”ì¸ ë£¨í”„) ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨. ì´ˆì•ˆì„ ê·¸ëŒ€ë¡œ ì±„íƒí•©ë‹ˆë‹¤.")
                    current_verified_summary = memory_draft
                    continue 

                # Step 4: ê²€ì¦ (Logic-based 7-Shot)
                verified_answers = self._execute_verification_plan(
                    questions=questions, 
                    current_session_context=session_context
                )
                
                # Step 5: ì¬êµ¬ì„± (Draft Correction)
                # ğŸ’¡ (í•µì‹¬ ìˆ˜ì •) old_memoryê°€ ì•„ë‹ˆë¼ memory_draftë¥¼ ë„˜ê¹ë‹ˆë‹¤.
                final_memory = self._reconstruct_final_memory(
                    draft_memory=memory_draft, 
                    verified_answers=verified_answers
                )
                
                current_verified_summary = final_memory
                print(f"\n--- â­ï¸ Session {session_number} ìµœì¢… ê²€ì¦ëœ ë©”ëª¨ë¦¬ â­ï¸ ---\n{current_verified_summary}\n----------------------------------")
            
            print(f"âœ… {len(all_sessions)}ê°œ ê³¼ê±° ì„¸ì…˜ì˜ ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸ ì™„ë£Œ.")

        print("ğŸ’¬ ìµœì¢… ì‘ë‹µ ìƒì„± ì¤‘... (ì»¨í…ìŠ¤íŠ¸: S_N)")
        final_memory_context = current_verified_summary if current_verified_summary.strip() else "none"
        
        system_prompt = self.response_system_prompt
        user_prompt = f"""**Instruction** {self.response_instruction}

**Test** [Previous Memory] {final_memory_context} [Dialogue Context] {current_context} [Response] 
"""
        return call_ollama_llm(system_prompt, user_prompt, "Final Response Generation", self.model_name)

if __name__ == "__main__":
    print("--- âš ï¸ ì´ íŒŒì¼ì€ 'Rsum_CoVe_ì‹¤í—˜êµ°' ëª¨ë“ˆì…ë‹ˆë‹¤. ---")
    print("ì‹¤í—˜ì„ ì‹¤í–‰í•˜ë ¤ë©´ 'run_realtalk_experiment.py'ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")


